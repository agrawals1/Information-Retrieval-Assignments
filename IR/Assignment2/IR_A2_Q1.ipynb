{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d6393ab",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7a4aebd",
   "metadata": {},
   "source": [
    "### TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe377176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13bcc14",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print file contents\n",
    "def printFileContent(path1, path2):\n",
    "    for file in os.listdir(path1)[:5]:\n",
    "        f1 = open(f'{path1}/{file}', 'r')\n",
    "        f2 = open(f\"{path2}/{file}\", 'r')\n",
    "        print(f\"\\nContent of file {file} before text extraction\\n\")\n",
    "        print(f1.read())    \n",
    "        print(f\"\\nContent of file {file} after text extraction\\n\")\n",
    "        print(f2.read())\n",
    "        print(\"\\n-----------------------------------------------------------\\n\")\n",
    "        f1.close()\n",
    "        f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14fe12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text between tags from all files\n",
    "def extractText(path):\n",
    "    for file in os.listdir(path):\n",
    "        f = open(f\"{path}/{file}\", 'r')\n",
    "        text = f.read() \n",
    "        f.close()\n",
    "        x = re.search(\"((?<=\\<TITLE\\>)(.|\\n)*(?=\\<\\/TITLE\\>))\", text)\n",
    "        y = re.search(\"((?<=\\<TEXT\\>)(.|\\n)*(?=\\<\\/TEXT\\>))\", text)\n",
    "\n",
    "        z = x.group(0).strip() + ' ' + y.group().strip()\n",
    "        z = \" \".join(z.split())\n",
    "        f = open(f\"{path}/{file}\", 'w')\n",
    "        f.write(z)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./Assignment1/CSE508_Winter2023_Dataset1/CSE508_Winter2023_Dataset\"\n",
    "path2 = \"./Assignment1/CSE508_Winter2023_Dataset/CSE508_Winter2023_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractText(path2)\n",
    "printFileContent(path1, path2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f51915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lowercase the text\n",
    "def toLowerCase(path,df):\n",
    "    for file in os.listdir(path):\n",
    "        f = open(f\"{path2}/{file}\", 'r')\n",
    "        text = f.read()\n",
    "        lower_text = text.lower()\n",
    "        df.loc[len(df.index)] = [file,text,lower_text]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing before and after lowercase\n",
    "lowercase_df = pd.DataFrame(columns = ['Doc Id','Before Lowercase','After Lowercase'])\n",
    "toLowerCase(path2,lowercase_df)\n",
    "lowercase_df.set_index('Doc Id',inplace = True)\n",
    "lowercase_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize the text\n",
    "def tokenizer(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        f = open(f\"{path2}/{file}\", 'r')\n",
    "        text = f.read()\n",
    "        tokens = word_tokenize(text)\n",
    "        tokenized_text = str(tokens)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Lowercase'],tokenized_text]\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78689527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing before and after tokenization\n",
    "tokenize_df = pd.DataFrame(columns = ['Doc Id','Before Tokenization','After Tokenization'])\n",
    "tokenizer(path2,tokenize_df,lowercase_df)\n",
    "tokenize_df.set_index('Doc Id',inplace = True)\n",
    "tokenize_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stop words\n",
    "def removeStopWords(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        tokens = ast.literal_eval(prev_df.at[file,'After Tokenization'])\n",
    "        remove_stop = [t for t in tokens if not t in stopwords.words(\"english\")]\n",
    "        res = str(remove_stop)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Tokenization'],res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing before and after stopwords removal\n",
    "stopwords_df = pd.DataFrame(columns = ['Doc Id','Before Stopwords Removal','After Stopwords Removal'])\n",
    "removeStopWords(path2,stopwords_df,tokenize_df)\n",
    "stopwords_df.set_index('Doc Id',inplace = True)\n",
    "stopwords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30cc9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove punctuation\n",
    "punctuations = string.punctuation\n",
    "punc = [ p for p in punctuations]\n",
    "\n",
    "def removePunctuation(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        tokens = ast.literal_eval(prev_df.at[file,'After Stopwords Removal'])\n",
    "        res = [t for t in tokens if t not in punc]\n",
    "        res = str(res)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Stopwords Removal'],res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing before and after punctuation removal\n",
    "pucntuation_df = pd.DataFrame(columns = ['Doc Id','Before Punctuation Removal','After Punctuation Removal'])\n",
    "removePunctuation(path2,pucntuation_df,stopwords_df)\n",
    "pucntuation_df.set_index('Doc Id',inplace = True)\n",
    "pucntuation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove blank space\n",
    "def removeBlanks(path,df,prev_df):\n",
    "    for file in os.listdir(path):\n",
    "        tokens = ast.literal_eval(prev_df.at[file,'After Punctuation Removal'])\n",
    "        res = [ t.strip() for t in tokens if len(t.strip()) != 0]\n",
    "        res = str(res)\n",
    "        df.loc[len(df.index)] = [file,prev_df.at[file,'After Punctuation Removal'],res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing before and after blank spaces removal\n",
    "blanks_df = pd.DataFrame(columns = ['Doc Id','Before Blank Space Tokens Removal',\n",
    "                                    'After Blank Space Tokens Removal'])\n",
    "removeBlanks(path2,blanks_df,pucntuation_df)\n",
    "blanks_df.set_index('Doc Id',inplace = True)\n",
    "blanks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final pre-processed data\n",
    "final_df = blanks_df.drop('Before Blank Space Tokens Removal', axis=1)\n",
    "final_df.rename(columns = {'After Blank Space Tokens Removal':'Contents of file after Preprocessing'}, \n",
    "                inplace = True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81519d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"Desktop/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2676b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1328b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Id</th>\n",
       "      <th>Contents of file after Preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield1223</td>\n",
       "      <td>['inviscid-incompressible-flow', 'theory', 'static', 'two-dimensional', 'solid', 'jets', 'proximity', 'ground', 'inviscid-incompressible-flow', 'theory', 'static', 'two-dimensional', 'solid', 'jets', 'impinging', 'orthogonally', 'ground', 'presented', 'using', 'conformal', 'mapping', 'methods', 'shown', 'thrust', 'solid', 'jet', 'constant', 'power', 'initially', 'decreases', 'ground', 'approached', 'magnitude', 'thrust', 'ground', 'effect', 'regained', 'low', 'height-to-jet', 'width', 'ratio', 'approximately', '0.55', 'maximuin', 'decrease', '6', 'percent', 'ground', 'effect', 'solid', 'jets', 'thus', 'largely', 'unfavorable']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield1011</td>\n",
       "      <td>['free-flight', 'measurements', 'static', 'dynamic', 'charts', 'prepared', 'relating', 'thermodynamic', 'properties', 'air', 'chemical', 'equilibrium', 'temperatures', '15,000', 'k', 'pressures', '10', '10', 'atmospheres', 'also', 'included', 'charts', 'showing', 'composition', 'air', 'isentropic', 'exponent', 'speed', 'sound', 'charts', 'based', 'thermodynamic', 'data', 'calculated', 'national', 'bureau', 'standards']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0795</td>\n",
       "      <td>['operation', 'npl', '18in', 'x', '14in', 'wind', 'tunnel', 'transonic', 'speed', 'range', 'brief', 'description', 'slotted', 'liners', 'used', 'given', 'together', 'power', 'requirements', 'flow', 'surveys', 'observations', 'made', 'wall', 'interference', 'half-model', 'swept', 'wing', 'tested', 'wind', 'tunnel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0553</td>\n",
       "      <td>['ablation', 'glassy', 'materials', 'around', 'blunt', 'bodies', 'revolution', 'steady-state', 'equations', 'motion', 'thin', 'layer', 'incompressible', 'glassy', 'material', 'surface', 'ablating', 'radiating', 'blunt', 'body', 'reduced', 'first-order', 'ordinary', 'differential', 'equation', 'integrated', 'numerically', 'solution', 'coupled', 'solution', 'air', 'boundary', 'layer', 'laminar', 'turbulent', 'heat', 'transfer', 'without', 'mass', 'vaporization', 'ablating', 'material', 'distribution', 'effective', 'energy', 'ablation', 'around', 'body', 'thus', 'obtained', 'cone', 'cylinder', 'hemispherical', 'cap', 're-enters', 'atmosphere', 'hypersonic', 'flight', 'speeds', 'quartz', 'ablating', 'material', 'found', 'ablation', 'process', 'turbulent', 'heating', 'efficient', 'laminar', 'case', 'increased', 'vaporization', 'solution', 'equations', 'motion', 'stagnation', 'point', 'verified', 'wind', 'tunnel', 'experiments', 'present', 'state', 'development', 'wind', 'tunnel', 'permit', 'use', 'experimental', 'investigations', 'ablation', 'around', 'blunt', 'bodies', 'turbulent', 'heating']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0761</td>\n",
       "      <td>['buckling', 'sandwich', 'normal', 'pressure', 'theoretical', 'study', 'made', 'buckling', 'sandwich', 'sphere', 'comprised', 'core', 'layer', 'low-modulus', 'material', 'two', 'thin', 'facing', 'layers', 'higher', 'modulus', 'material', 'solution', 'buckling', 'resistance', 'sphere', 'normal', 'external', 'pressure', 'obtained', 'linearized', 'theory', 'reducible', 'classical', 'solution', 'monocoque', 'spherical', 'shells', 'critical', 'buckling', 'pressures', 'calculated', 'various', 'radius-thickness', 'ratios', 'sphere', 'materials']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_Id  \\\n",
       "0  cranfield1223   \n",
       "1  cranfield1011   \n",
       "2  cranfield0795   \n",
       "3  cranfield0553   \n",
       "4  cranfield0761   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Contents of file after Preprocessing  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ['inviscid-incompressible-flow', 'theory', 'static', 'two-dimensional', 'solid', 'jets', 'proximity', 'ground', 'inviscid-incompressible-flow', 'theory', 'static', 'two-dimensional', 'solid', 'jets', 'impinging', 'orthogonally', 'ground', 'presented', 'using', 'conformal', 'mapping', 'methods', 'shown', 'thrust', 'solid', 'jet', 'constant', 'power', 'initially', 'decreases', 'ground', 'approached', 'magnitude', 'thrust', 'ground', 'effect', 'regained', 'low', 'height-to-jet', 'width', 'ratio', 'approximately', '0.55', 'maximuin', 'decrease', '6', 'percent', 'ground', 'effect', 'solid', 'jets', 'thus', 'largely', 'unfavorable']  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ['free-flight', 'measurements', 'static', 'dynamic', 'charts', 'prepared', 'relating', 'thermodynamic', 'properties', 'air', 'chemical', 'equilibrium', 'temperatures', '15,000', 'k', 'pressures', '10', '10', 'atmospheres', 'also', 'included', 'charts', 'showing', 'composition', 'air', 'isentropic', 'exponent', 'speed', 'sound', 'charts', 'based', 'thermodynamic', 'data', 'calculated', 'national', 'bureau', 'standards']  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ['operation', 'npl', '18in', 'x', '14in', 'wind', 'tunnel', 'transonic', 'speed', 'range', 'brief', 'description', 'slotted', 'liners', 'used', 'given', 'together', 'power', 'requirements', 'flow', 'surveys', 'observations', 'made', 'wall', 'interference', 'half-model', 'swept', 'wing', 'tested', 'wind', 'tunnel']  \n",
       "3  ['ablation', 'glassy', 'materials', 'around', 'blunt', 'bodies', 'revolution', 'steady-state', 'equations', 'motion', 'thin', 'layer', 'incompressible', 'glassy', 'material', 'surface', 'ablating', 'radiating', 'blunt', 'body', 'reduced', 'first-order', 'ordinary', 'differential', 'equation', 'integrated', 'numerically', 'solution', 'coupled', 'solution', 'air', 'boundary', 'layer', 'laminar', 'turbulent', 'heat', 'transfer', 'without', 'mass', 'vaporization', 'ablating', 'material', 'distribution', 'effective', 'energy', 'ablation', 'around', 'body', 'thus', 'obtained', 'cone', 'cylinder', 'hemispherical', 'cap', 're-enters', 'atmosphere', 'hypersonic', 'flight', 'speeds', 'quartz', 'ablating', 'material', 'found', 'ablation', 'process', 'turbulent', 'heating', 'efficient', 'laminar', 'case', 'increased', 'vaporization', 'solution', 'equations', 'motion', 'stagnation', 'point', 'verified', 'wind', 'tunnel', 'experiments', 'present', 'state', 'development', 'wind', 'tunnel', 'permit', 'use', 'experimental', 'investigations', 'ablation', 'around', 'blunt', 'bodies', 'turbulent', 'heating']  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ['buckling', 'sandwich', 'normal', 'pressure', 'theoretical', 'study', 'made', 'buckling', 'sandwich', 'sphere', 'comprised', 'core', 'layer', 'low-modulus', 'material', 'two', 'thin', 'facing', 'layers', 'higher', 'modulus', 'material', 'solution', 'buckling', 'resistance', 'sphere', 'normal', 'external', 'pressure', 'obtained', 'linearized', 'theory', 'reducible', 'classical', 'solution', 'monocoque', 'spherical', 'shells', 'critical', 'buckling', 'pressures', 'calculated', 'various', 'radius-thickness', 'ratios', 'sphere', 'materials']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming column\n",
    "df.rename(columns={'Doc Id':'Doc_Id'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6818b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making word corpus of all the words in all the documents\n",
    "word_corpus = set()\n",
    "for i in range(0,len(df)):\n",
    "    content = ast.literal_eval(df['Contents of file after Preprocessing'][i])\n",
    "    for token in content:\n",
    "        word_corpus.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846fe8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf dictionary with Binary Weighting Scheme\n",
    "binary_tf = {}\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    content = ast.literal_eval(df['Contents of file after Preprocessing'][i])\n",
    "    doc_id = df['Doc_Id'][i]\n",
    "    for term in content:\n",
    "        if(term not in binary_tf):\n",
    "            binary_tf[term] = {}\n",
    "        binary_tf[term][doc_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f39cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf dictionary with Raw count Weighting Scheme\n",
    "raw_tf = {}\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    content = ast.literal_eval(df['Contents of file after Preprocessing'][i])\n",
    "    doc_id = df['Doc_Id'][i]\n",
    "    for term in content:\n",
    "        if(term not in raw_tf):\n",
    "            raw_tf[term] = {}\n",
    "            raw_tf[term][doc_id] = 1\n",
    "        else:\n",
    "            if(doc_id not in raw_tf[term]):\n",
    "                raw_tf[term][doc_id] = 1\n",
    "            else:\n",
    "                raw_tf[term][doc_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba949515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf dictionary with Term frequency Weighting Scheme\n",
    "term_tf = deepcopy(raw_tf)\n",
    "\n",
    "for terms in term_tf:\n",
    "    doc_freq_dict = term_tf[terms]\n",
    "    sum_freq = 0\n",
    "    for docs in doc_freq_dict:\n",
    "        sum_freq += doc_freq_dict[docs]\n",
    "    for docs in doc_freq_dict:\n",
    "        doc_freq_dict[docs] = round((doc_freq_dict[docs]/sum_freq), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c6d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf dictionary with Log normalization Weighting Scheme\n",
    "log_tf = deepcopy(raw_tf)\n",
    "\n",
    "for terms in log_tf:\n",
    "    doc_freq_dict = log_tf[terms]\n",
    "    for docs in doc_freq_dict:\n",
    "         doc_freq_dict[docs] = math.log((1+doc_freq_dict[docs]),10)\n",
    "         doc_freq_dict[docs] = round(doc_freq_dict[docs] , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "718ef9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf dictionary with Double normalization Weighting Scheme\n",
    "double_tf = deepcopy(raw_tf)\n",
    "\n",
    "for terms in double_tf:\n",
    "    doc_freq_dict = double_tf[terms]\n",
    "    max_freq = 0\n",
    "    for docs in doc_freq_dict:\n",
    "        max_freq = max(max_freq,doc_freq_dict[docs])\n",
    "    for docs in doc_freq_dict:\n",
    "        doc_freq_dict[docs] = 0.5 + 0.5*(doc_freq_dict[docs]/max_freq)\n",
    "        doc_freq_dict[docs] = round(doc_freq_dict[docs] , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b14a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating unigram inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "for word in word_corpus:\n",
    "    for docId in df[\"Doc_Id\"]:\n",
    "        if(word in ast.literal_eval(df.loc[df[\"Doc_Id\"] == docId, \"Contents of file after Preprocessing\"].iloc[0])):\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = []\n",
    "            if word in inverted_index and docId not in inverted_index[word]:\n",
    "                inverted_index[word].append(docId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461b126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pickle to load inverted index\n",
    "file = open(\"/Users/amritaaash/Desktop/inverted_index\", 'rb')\n",
    "inverted_index = pickle.load(file)\n",
    "for key,val in inverted_index.items():\n",
    "    val.sort()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535a0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of documents\n",
    "tot_docs = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b1b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating idf\n",
    "idf = {}\n",
    "\n",
    "for term in inverted_index:\n",
    "    list_docs = inverted_index[term]\n",
    "    temp = tot_docs/(len(list_docs)+1)\n",
    "    temp = math.log(temp,10)\n",
    "    temp = round(temp,4)\n",
    "    idf[term] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7403338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with Binary Weighting Scheme as tf\n",
    "binary_tfidf_df = deepcopy(df)\n",
    "binary_tfidf_df.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c8555d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing tf-idf dataframe with Binary Weighting Scheme as tf\n",
    "for term in word_corpus:\n",
    "    term_idf = 0\n",
    "    if(term in idf):\n",
    "        term_idf = idf[term]\n",
    "    term_dict = binary_tf[term]\n",
    "    tf_idf_list = []\n",
    "    for doc_id in range(0,len(binary_tfidf_df)):\n",
    "        tf_term = 0\n",
    "        if(binary_tfidf_df['Doc_Id'][doc_id] in term_dict):\n",
    "            tf_term = term_dict[binary_tfidf_df['Doc_Id'][doc_id]]\n",
    "        tf_idf = tf_term * term_idf\n",
    "        tf_idf_list.append(tf_idf)\n",
    "    binary_tfidf_df[term] = tf_idf_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760a9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Id</th>\n",
       "      <th>shielded</th>\n",
       "      <th>assembled</th>\n",
       "      <th>300</th>\n",
       "      <th>3.2</th>\n",
       "      <th>read</th>\n",
       "      <th>observe</th>\n",
       "      <th>steady</th>\n",
       "      <th>mound</th>\n",
       "      <th>sheltered</th>\n",
       "      <th>...</th>\n",
       "      <th>arbitrary</th>\n",
       "      <th>discrete</th>\n",
       "      <th>ordnance</th>\n",
       "      <th>remember</th>\n",
       "      <th>cut-off</th>\n",
       "      <th>recognised</th>\n",
       "      <th>hyperbolic</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>near-field</th>\n",
       "      <th>height-thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield1223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield1011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_Id  shielded  assembled  300  3.2  read  observe  steady  mound  \\\n",
       "0  cranfield1223       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "1  cranfield1011       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "2  cranfield0795       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "3  cranfield0553       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "4  cranfield0761       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "\n",
       "   sheltered  ...  arbitrary  discrete  ordnance  remember  cut-off  \\\n",
       "0        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "1        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "2        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "3        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "4        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "\n",
       "   recognised  hyperbolic  augmentation  near-field  height-thickness  \n",
       "0         0.0         0.0           0.0         0.0               0.0  \n",
       "1         0.0         0.0           0.0         0.0               0.0  \n",
       "2         0.0         0.0           0.0         0.0               0.0  \n",
       "3         0.0         0.0           0.0         0.0               0.0  \n",
       "4         0.0         0.0           0.0         0.0               0.0  \n",
       "\n",
       "[5 rows x 9710 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "717a42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping tf-idf dataframe using pickle\n",
    "file = open(\"/Users/amritaaash/Desktop/binary_tfidf_df\", 'wb')\n",
    "pickle.dump(binary_tfidf_df, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8ed0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with Raw Count Weighting Scheme as tf\n",
    "raw_tfidf_df = deepcopy(df)\n",
    "raw_tfidf_df.drop('Contents of file after Preprocessing',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c5a9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amritaaash/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# constructing tf-idf dataframe with Raw Count Weighting Scheme as tf\n",
    "for term in word_corpus:\n",
    "    term_idf = 0\n",
    "    if(term in idf):\n",
    "        term_idf = idf[term]\n",
    "    term_dict = raw_tf[term]\n",
    "    tf_idf_list = []\n",
    "    if(term_idf == 0):\n",
    "        raw_tfidf_df[term] = [0] * len(raw_tfidf_df)\n",
    "        continue\n",
    "    for doc_id in range(0,len(raw_tfidf_df)):\n",
    "        tf_term = 0\n",
    "        if(raw_tfidf_df['Doc_Id'][doc_id] in term_dict):\n",
    "            tf_term = term_dict[raw_tfidf_df['Doc_Id'][doc_id]]\n",
    "        tf_idf = tf_term * term_idf\n",
    "        tf_idf_list.append(tf_idf)\n",
    "    raw_tfidf_df[term] = tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f7fb879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Id</th>\n",
       "      <th>shielded</th>\n",
       "      <th>assembled</th>\n",
       "      <th>300</th>\n",
       "      <th>3.2</th>\n",
       "      <th>read</th>\n",
       "      <th>observe</th>\n",
       "      <th>steady</th>\n",
       "      <th>mound</th>\n",
       "      <th>sheltered</th>\n",
       "      <th>...</th>\n",
       "      <th>arbitrary</th>\n",
       "      <th>discrete</th>\n",
       "      <th>ordnance</th>\n",
       "      <th>remember</th>\n",
       "      <th>cut-off</th>\n",
       "      <th>recognised</th>\n",
       "      <th>hyperbolic</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>near-field</th>\n",
       "      <th>height-thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield1223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield1011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_Id  shielded  assembled  300  3.2  read  observe  steady  mound  \\\n",
       "0  cranfield1223       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "1  cranfield1011       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "2  cranfield0795       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "3  cranfield0553       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "4  cranfield0761       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "\n",
       "   sheltered  ...  arbitrary  discrete  ordnance  remember  cut-off  \\\n",
       "0        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "1        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "2        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "3        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "4        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "\n",
       "   recognised  hyperbolic  augmentation  near-field  height-thickness  \n",
       "0         0.0         0.0           0.0         0.0               0.0  \n",
       "1         0.0         0.0           0.0         0.0               0.0  \n",
       "2         0.0         0.0           0.0         0.0               0.0  \n",
       "3         0.0         0.0           0.0         0.0               0.0  \n",
       "4         0.0         0.0           0.0         0.0               0.0  \n",
       "\n",
       "[5 rows x 9710 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2dceecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping tf-idf dataframe using pickle\n",
    "file = open(\"/Users/amritaaash/Desktop/raw_tfidf_df\", 'wb')\n",
    "pickle.dump(raw_tfidf_df, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60800427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with Term frequency Weighting Scheme as tf\n",
    "term_tfidf_df = deepcopy(df)\n",
    "term_tfidf_df.drop('Contents of file after Preprocessing',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8122132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amritaaash/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# constructing tf-idf dataframe with Term Frequency Weighting Scheme as tf\n",
    "for term in word_corpus:\n",
    "    term_idf = 0\n",
    "    if(term in idf):\n",
    "        term_idf = idf[term]\n",
    "    term_dict = term_tf[term]\n",
    "    tf_idf_list = []\n",
    "    if(term_idf == 0):\n",
    "        term_tfidf_df[term] = [0] * len(term_tfidf_df)\n",
    "        continue\n",
    "    for doc_id in range(0,len(term_tfidf_df)):\n",
    "        tf_term = 0\n",
    "        if(term_tfidf_df['Doc_Id'][doc_id] in term_dict):\n",
    "            tf_term = term_dict[term_tfidf_df['Doc_Id'][doc_id]]\n",
    "        tf_idf = tf_term * term_idf\n",
    "        tf_idf_list.append(tf_idf)\n",
    "    term_tfidf_df[term] = tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "852b1284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Id</th>\n",
       "      <th>shielded</th>\n",
       "      <th>assembled</th>\n",
       "      <th>300</th>\n",
       "      <th>3.2</th>\n",
       "      <th>read</th>\n",
       "      <th>observe</th>\n",
       "      <th>steady</th>\n",
       "      <th>mound</th>\n",
       "      <th>sheltered</th>\n",
       "      <th>...</th>\n",
       "      <th>arbitrary</th>\n",
       "      <th>discrete</th>\n",
       "      <th>ordnance</th>\n",
       "      <th>remember</th>\n",
       "      <th>cut-off</th>\n",
       "      <th>recognised</th>\n",
       "      <th>hyperbolic</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>near-field</th>\n",
       "      <th>height-thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield1223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield1011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_Id  shielded  assembled  300  3.2  read  observe  steady  mound  \\\n",
       "0  cranfield1223       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "1  cranfield1011       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "2  cranfield0795       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "3  cranfield0553       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "4  cranfield0761       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "\n",
       "   sheltered  ...  arbitrary  discrete  ordnance  remember  cut-off  \\\n",
       "0        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "1        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "2        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "3        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "4        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "\n",
       "   recognised  hyperbolic  augmentation  near-field  height-thickness  \n",
       "0         0.0         0.0           0.0         0.0               0.0  \n",
       "1         0.0         0.0           0.0         0.0               0.0  \n",
       "2         0.0         0.0           0.0         0.0               0.0  \n",
       "3         0.0         0.0           0.0         0.0               0.0  \n",
       "4         0.0         0.0           0.0         0.0               0.0  \n",
       "\n",
       "[5 rows x 9710 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea97fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping tf-idf dataframe using pickle\n",
    "file = open(\"/Users/amritaaash/Desktop/term_tfidf_df\", 'wb')\n",
    "pickle.dump(term_tfidf_df, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7094809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with Log normalization Weighting Scheme as tf\n",
    "log_tfidf_df = deepcopy(df)\n",
    "log_tfidf_df.drop('Contents of file after Preprocessing',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "255db792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amritaaash/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# constructing tf-idf dataframe with Term Frequency Weighting Scheme as tf\n",
    "for term in word_corpus:\n",
    "    term_idf = 0\n",
    "    if(term in idf):\n",
    "        term_idf = idf[term]\n",
    "    term_dict = log_tf[term]\n",
    "    tf_idf_list = []\n",
    "    if(term_idf == 0):\n",
    "        log_tfidf_df[term] = [0] * len(log_tfidf_df)\n",
    "        continue\n",
    "    for doc_id in range(0,len(log_tfidf_df)):\n",
    "        tf_term = 0\n",
    "        if(log_tfidf_df['Doc_Id'][doc_id] in term_dict):\n",
    "            tf_term = term_dict[log_tfidf_df['Doc_Id'][doc_id]]\n",
    "        tf_idf = tf_term * term_idf\n",
    "        tf_idf_list.append(tf_idf)\n",
    "    log_tfidf_df[term] = tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ac5c355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Id</th>\n",
       "      <th>shielded</th>\n",
       "      <th>assembled</th>\n",
       "      <th>300</th>\n",
       "      <th>3.2</th>\n",
       "      <th>read</th>\n",
       "      <th>observe</th>\n",
       "      <th>steady</th>\n",
       "      <th>mound</th>\n",
       "      <th>sheltered</th>\n",
       "      <th>...</th>\n",
       "      <th>arbitrary</th>\n",
       "      <th>discrete</th>\n",
       "      <th>ordnance</th>\n",
       "      <th>remember</th>\n",
       "      <th>cut-off</th>\n",
       "      <th>recognised</th>\n",
       "      <th>hyperbolic</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>near-field</th>\n",
       "      <th>height-thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield1223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield1011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_Id  shielded  assembled  300  3.2  read  observe  steady  mound  \\\n",
       "0  cranfield1223       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "1  cranfield1011       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "2  cranfield0795       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "3  cranfield0553       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "4  cranfield0761       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "\n",
       "   sheltered  ...  arbitrary  discrete  ordnance  remember  cut-off  \\\n",
       "0        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "1        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "2        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "3        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "4        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "\n",
       "   recognised  hyperbolic  augmentation  near-field  height-thickness  \n",
       "0         0.0         0.0           0.0         0.0               0.0  \n",
       "1         0.0         0.0           0.0         0.0               0.0  \n",
       "2         0.0         0.0           0.0         0.0               0.0  \n",
       "3         0.0         0.0           0.0         0.0               0.0  \n",
       "4         0.0         0.0           0.0         0.0               0.0  \n",
       "\n",
       "[5 rows x 9710 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e473bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping tf-idf dataframe using pickle\n",
    "file = open(\"/Users/amritaaash/Desktop/log_tfidf_df\", 'wb')\n",
    "pickle.dump(log_tfidf_df, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edd100c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with Log normalization Weighting Scheme as tf\n",
    "double_tfidf_df = deepcopy(df)\n",
    "double_tfidf_df.drop('Contents of file after Preprocessing',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8d2a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amritaaash/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# constructing tf-idf dataframe with Term Frequency Weighting Scheme as tf\n",
    "for term in word_corpus:\n",
    "    term_idf = 0\n",
    "    if(term in idf):\n",
    "        term_idf = idf[term]\n",
    "    term_dict = double_tf[term]\n",
    "    tf_idf_list = []\n",
    "    if(term_idf == 0):\n",
    "        double_tfidf_df[term] = [0] * len(double_tfidf_df)\n",
    "        continue\n",
    "    for doc_id in range(0,len(double_tfidf_df)):\n",
    "        tf_term = 0\n",
    "        if(double_tfidf_df['Doc_Id'][doc_id] in term_dict):\n",
    "            tf_term = term_dict[double_tfidf_df['Doc_Id'][doc_id]]\n",
    "        tf_idf = tf_term * term_idf\n",
    "        tf_idf_list.append(tf_idf)\n",
    "    double_tfidf_df[term] = tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d0b1eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_Id</th>\n",
       "      <th>shielded</th>\n",
       "      <th>assembled</th>\n",
       "      <th>300</th>\n",
       "      <th>3.2</th>\n",
       "      <th>read</th>\n",
       "      <th>observe</th>\n",
       "      <th>steady</th>\n",
       "      <th>mound</th>\n",
       "      <th>sheltered</th>\n",
       "      <th>...</th>\n",
       "      <th>arbitrary</th>\n",
       "      <th>discrete</th>\n",
       "      <th>ordnance</th>\n",
       "      <th>remember</th>\n",
       "      <th>cut-off</th>\n",
       "      <th>recognised</th>\n",
       "      <th>hyperbolic</th>\n",
       "      <th>augmentation</th>\n",
       "      <th>near-field</th>\n",
       "      <th>height-thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cranfield1223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cranfield1011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranfield0795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cranfield0553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cranfield0761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9710 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Doc_Id  shielded  assembled  300  3.2  read  observe  steady  mound  \\\n",
       "0  cranfield1223       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "1  cranfield1011       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "2  cranfield0795       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "3  cranfield0553       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "4  cranfield0761       0.0        0.0  0.0  0.0   0.0      0.0     0.0    0.0   \n",
       "\n",
       "   sheltered  ...  arbitrary  discrete  ordnance  remember  cut-off  \\\n",
       "0        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "1        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "2        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "3        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "4        0.0  ...        0.0       0.0       0.0       0.0      0.0   \n",
       "\n",
       "   recognised  hyperbolic  augmentation  near-field  height-thickness  \n",
       "0         0.0         0.0           0.0         0.0               0.0  \n",
       "1         0.0         0.0           0.0         0.0               0.0  \n",
       "2         0.0         0.0           0.0         0.0               0.0  \n",
       "3         0.0         0.0           0.0         0.0               0.0  \n",
       "4         0.0         0.0           0.0         0.0               0.0  \n",
       "\n",
       "[5 rows x 9710 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8e5372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping tf-idf dataframe using pickle\n",
    "file = open(\"/Users/amritaaash/Desktop/double_tfidf_df\", 'wb')\n",
    "pickle.dump(double_tfidf_df, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d13ba474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing query\n",
    "def preprocess_query(query):\n",
    "    lower_query = query.lower()\n",
    "    tokens_query = word_tokenize(lower_query)\n",
    "    remove_stop_query = [t for t in tokens_query if not t in stopwords.words(\"english\")]\n",
    "    remove_punc_query = [t for t in remove_stop_query if t not in punc]\n",
    "    remove_blank_query = [ t.strip() for t in remove_punc_query if len(t.strip()) != 0]\n",
    "    preprocessed_query = remove_blank_query\n",
    "    return preprocessed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dc4ea86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating idf of query\n",
    "def compute_query_idf(preprocessed_query):\n",
    "    query_idf = {}\n",
    "\n",
    "    for word in word_corpus:\n",
    "        if(word in preprocessed_query):\n",
    "            if(word in idf):\n",
    "                query_idf[word] = idf[word]\n",
    "            else:\n",
    "                query_idf[word] = 0\n",
    "        else:\n",
    "            query_idf[word] = 0\n",
    "    return query_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a45e27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf of query with Binary Weighting Scheme\n",
    "def compute_binary_query_tf(preprocessed_query):\n",
    "    binary_query_tf = {}\n",
    "\n",
    "    for word in word_corpus:\n",
    "        if(word in preprocessed_query):\n",
    "            binary_query_tf[word] = 1\n",
    "        else:\n",
    "            binary_query_tf[word] = 0\n",
    "\n",
    "    return binary_query_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "838ec2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf of query with Raw Count Weighting Scheme\n",
    "def compute_raw_query_tf(preprocessed_query):\n",
    "    raw_query_tf = {}\n",
    "\n",
    "    for word in word_corpus:\n",
    "        if(word in preprocessed_query):\n",
    "            if(word in raw_query_tf):\n",
    "                raw_query_tf[word] += 1\n",
    "            else:\n",
    "                raw_query_tf[word] = 1\n",
    "        else:\n",
    "            raw_query_tf[word] = 0\n",
    "\n",
    "    return raw_query_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "78db6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf of query with Term Frequency Weighting Scheme\n",
    "def compute_term_query_tf(raw_query_tf):\n",
    "    term_query_tf = deepcopy(raw_query_tf)\n",
    "\n",
    "    s=0\n",
    "    for term in term_query_tf:\n",
    "        s += term_query_tf[term]\n",
    "    \n",
    "    for term in term_query_tf:\n",
    "        term_query_tf[term] = round((term_query_tf[term]/s),4)\n",
    "\n",
    "    return term_query_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1525e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf of query with Log Normalization Weighting Scheme\n",
    "def compute_log_query_tf(raw_query_tf):\n",
    "    log_query_tf = deepcopy(raw_query_tf)\n",
    "    \n",
    "    for term in log_query_tf:\n",
    "        log_query_tf[term] = math.log((1 + log_query_tf[term]))\n",
    "        log_query_tf[term] = round((log_query_tf[term]),4)\n",
    "\n",
    "    return log_query_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bd4d2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing tf of query with Double Normalization Weighting Scheme\n",
    "def compute_double_query_tf(raw_query_tf):\n",
    "    double_query_tf = deepcopy(raw_query_tf)\n",
    "\n",
    "    maxi = 0\n",
    "    for term in double_query_tf:\n",
    "        maxi = max(maxi,double_query_tf[term])\n",
    "    \n",
    "    for term in double_query_tf:\n",
    "        double_query_tf[term] = 0.5 * (double_query_tf[term]/maxi)\n",
    "        double_query_tf[term] = 0.5 + double_query_tf[term]\n",
    "        double_query_tf[term] = round((double_query_tf[term]),4)\n",
    "\n",
    "    return double_query_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0fc9a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of query and document using Binary Weighting Scheme\n",
    "def binary(query_tfidf_list):\n",
    "    file = open(\"/Users/amritaaash/Desktop/binary_tfidf_df\", 'rb')\n",
    "    binary_tfidf_df = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    query_df_score = deepcopy(df)\n",
    "    query_df_score.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n",
    "\n",
    "    cos_sim_col = []\n",
    "    for index,rows in binary_tfidf_df.iterrows():\n",
    "        row_vec = list(rows)[1:]\n",
    "        query_vec = query_tfidf_list\n",
    "        row_vec = np.array(row_vec)\n",
    "        query_vec = np.array(query_vec)\n",
    "        cos_sim = np.dot(row_vec,query_vec)/(norm(row_vec)*norm(query_vec))\n",
    "        cos_sim = round(cos_sim,4)\n",
    "        cos_sim_col.append(cos_sim)\n",
    "    query_df_score[\"Cosine_Similarity\"] = cos_sim_col\n",
    "\n",
    "    query_df_score.sort_values(\"Cosine_Similarity\",ascending=False,inplace=True)\n",
    "\n",
    "    top_5_binary = query_df_score.iloc[:5]\n",
    "    \n",
    "    return top_5_binary\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "920d9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of query and document using Raw Count Weighting Scheme\n",
    "def raw(query_tfidf_list):\n",
    "    file = open(\"/Users/amritaaash/Desktop/raw_tfidf_df\", 'rb')\n",
    "    raw_tfidf_df = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    query_df_score = deepcopy(df)\n",
    "    query_df_score.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n",
    "\n",
    "    cos_sim_col = []\n",
    "    for index,rows in raw_tfidf_df.iterrows():\n",
    "        row_vec = list(rows)[1:]\n",
    "        query_vec = query_tfidf_list\n",
    "        row_vec = np.array(row_vec)\n",
    "        query_vec = np.array(query_vec)\n",
    "        cos_sim = np.dot(row_vec,query_vec)/(norm(row_vec)*norm(query_vec))\n",
    "        cos_sim = round(cos_sim,4)\n",
    "        cos_sim_col.append(cos_sim)\n",
    "    query_df_score[\"Cosine_Similarity\"] = cos_sim_col\n",
    "\n",
    "    query_df_score.sort_values(\"Cosine_Similarity\",ascending=False,inplace=True)\n",
    "\n",
    "    top_5_raw = query_df_score[:5]\n",
    "    \n",
    "    return top_5_raw\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7da00398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of query and document using Term Frequency Weighting Scheme\n",
    "def term_frequency(query_tfidf_list):\n",
    "    file = open(\"/Users/amritaaash/Desktop/term_tfidf_df\", 'rb')\n",
    "    term_tfidf_df = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    query_df_score = deepcopy(df)\n",
    "    query_df_score.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n",
    "\n",
    "    cos_sim_col = []\n",
    "    for index,rows in term_tfidf_df.iterrows():\n",
    "        row_vec = list(rows)[1:]\n",
    "        query_vec = query_tfidf_list\n",
    "        row_vec = np.array(row_vec)\n",
    "        query_vec = np.array(query_vec)\n",
    "        cos_sim = np.dot(row_vec,query_vec)/(norm(row_vec)*norm(query_vec))\n",
    "        cos_sim = round(cos_sim,4)\n",
    "        cos_sim_col.append(cos_sim)\n",
    "    query_df_score[\"Cosine_Similarity\"] = cos_sim_col\n",
    "\n",
    "    query_df_score.sort_values(\"Cosine_Similarity\",ascending=False,inplace=True)\n",
    "\n",
    "    top_5_tf = query_df_score[:5]\n",
    "    \n",
    "    return top_5_tf\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "afda7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of query and document using Log Normalization Weighting Scheme\n",
    "def log_normalization(query_tfidf_list):\n",
    "    file = open(\"/Users/amritaaash/Desktop/log_tfidf_df\", 'rb')\n",
    "    log_tfidf_df = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    query_df_score = deepcopy(df)\n",
    "    query_df_score.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n",
    "\n",
    "    cos_sim_col = []\n",
    "    for index,rows in log_tfidf_df.iterrows():\n",
    "        row_vec = list(rows)[1:]\n",
    "        query_vec = query_tfidf_list\n",
    "        row_vec = np.array(row_vec)\n",
    "        query_vec = np.array(query_vec)\n",
    "        cos_sim = np.dot(row_vec,query_vec)/(norm(row_vec)*norm(query_vec))\n",
    "        cos_sim = round(cos_sim,4)\n",
    "        cos_sim_col.append(cos_sim)\n",
    "    query_df_score[\"Cosine_Similarity\"] = cos_sim_col\n",
    "\n",
    "    query_df_score.sort_values(\"Cosine_Similarity\",ascending=False,inplace=True)\n",
    "\n",
    "    top_5_log = query_df_score[:5]\n",
    "    \n",
    "    return top_5_log\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bc34391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of query and document using Double Normalization Weighting Scheme\n",
    "def double_normalization(query_tfidf_list):\n",
    "    file = open(\"/Users/amritaaash/Desktop/double_tfidf_df\", 'rb')\n",
    "    double_tfidf_df = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    query_df_score = deepcopy(df)\n",
    "    query_df_score.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n",
    "\n",
    "    cos_sim_col = []\n",
    "    for index,rows in double_tfidf_df.iterrows():\n",
    "        row_vec = list(rows)[1:]\n",
    "        query_vec = query_tfidf_list\n",
    "        row_vec = np.array(row_vec)\n",
    "        query_vec = np.array(query_vec)\n",
    "        cos_sim = np.dot(row_vec,query_vec)/(norm(row_vec)*norm(query_vec))\n",
    "        cos_sim = round(cos_sim,4)\n",
    "        cos_sim_col.append(cos_sim)\n",
    "    query_df_score[\"Cosine_Similarity\"] = cos_sim_col\n",
    "\n",
    "    query_df_score.sort_values(\"Cosine_Similarity\",ascending=False,inplace=True)\n",
    "\n",
    "    top_5_double = query_df_score[:5]\n",
    "    \n",
    "    return top_5_double\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "29650f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_docs(query):\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    query_idf = compute_query_idf(preprocessed_query)\n",
    "\n",
    "\n",
    "    binary_query_tf = compute_binary_query_tf(preprocessed_query)\n",
    "\n",
    "    query_tfidf = {}\n",
    "    for word in query_idf:\n",
    "        query_tfidf[word] = binary_query_tf[word] * query_idf[word]\n",
    "    \n",
    "    query_tfidf_list = []\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf_list.append(query_tfidf[term])\n",
    "    \n",
    "    top_5_binary = binary(query_tfidf_list)\n",
    "    print(\"Top 5 documents according to Binary Weighting Scheme: \\n\")\n",
    "    top_5_binary.set_index('Doc_Id',inplace=True)\n",
    "    print(top_5_binary)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    raw_query_tf = compute_raw_query_tf(preprocessed_query)\n",
    "\n",
    "    query_tfidf = {}\n",
    "    for word in query_idf:\n",
    "        query_tfidf[word] = raw_query_tf[word] * query_idf[word]\n",
    "    \n",
    "    query_tfidf_list = []\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf_list.append(query_tfidf[term])\n",
    "\n",
    "    top_5_raw = raw(query_tfidf_list)\n",
    "    print(\"Top 5 documents according to Raw Weighting Scheme: \\n\")\n",
    "    top_5_raw.set_index('Doc_Id',inplace=True)\n",
    "    print(top_5_raw)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    term_query_tf = compute_term_query_tf(raw_query_tf)\n",
    "\n",
    "    query_tfidf = {}\n",
    "    for word in query_idf:\n",
    "        query_tfidf[word] = term_query_tf[word] * query_idf[word]\n",
    "    \n",
    "    query_tfidf_list = []\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf_list.append(query_tfidf[term])\n",
    "\n",
    "    top_5_tf = term_frequency(query_tfidf_list)\n",
    "    print(\"Top 5 documents according to Term Frequency Weighting Scheme: \\n\")\n",
    "    top_5_tf.set_index('Doc_Id',inplace=True)\n",
    "    print(top_5_tf)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    log_query_tf = compute_log_query_tf(raw_query_tf)\n",
    "\n",
    "    query_tfidf = {}\n",
    "    for word in query_idf:\n",
    "        query_tfidf[word] = log_query_tf[word] * query_idf[word]\n",
    "    \n",
    "    query_tfidf_list = []\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf_list.append(query_tfidf[term])\n",
    "\n",
    "    top_5_log = log_normalization(query_tfidf_list)\n",
    "    print(\"Top 5 documents according to Log Normalization Weighting Scheme: \\n\")\n",
    "    top_5_log.set_index('Doc_Id',inplace=True)\n",
    "    print(top_5_log)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    double_query_tf = compute_double_query_tf(raw_query_tf)\n",
    "\n",
    "    query_tfidf = {}\n",
    "    for word in query_idf:\n",
    "        query_tfidf[word] = double_query_tf[word] * query_idf[word]\n",
    "    \n",
    "    query_tfidf_list = []\n",
    "    for term in query_tfidf:\n",
    "        query_tfidf_list.append(query_tfidf[term])\n",
    "\n",
    "\n",
    "    top_5_double = double_normalization(query_tfidf_list)\n",
    "    print(\"Top 5 documents according to Double Normalization Weighting Scheme: \\n\")\n",
    "    top_5_double.set_index('Doc_Id',inplace=True)\n",
    "    print(top_5_double)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bf9e5af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents according to Binary Weighting Scheme: \n",
      "\n",
      "               Cosine_Similarity\n",
      "Doc_Id                          \n",
      "cranfield0001             0.1749\n",
      "cranfield0634             0.1581\n",
      "cranfield0011             0.1495\n",
      "cranfield0284             0.1473\n",
      "cranfield0360             0.1386\n",
      "\n",
      "\n",
      "Top 5 documents according to Raw Weighting Scheme: \n",
      "\n",
      "               Cosine_Similarity\n",
      "Doc_Id                          \n",
      "cranfield0634             0.2104\n",
      "cranfield0001             0.1244\n",
      "cranfield0011             0.1241\n",
      "cranfield0284             0.1193\n",
      "cranfield0372             0.1178\n",
      "\n",
      "\n",
      "Top 5 documents according to Term Frequency Weighting Scheme: \n",
      "\n",
      "               Cosine_Similarity\n",
      "Doc_Id                          \n",
      "cranfield0284             0.0187\n",
      "cranfield0634             0.0183\n",
      "cranfield1331             0.0151\n",
      "cranfield0753             0.0151\n",
      "cranfield0360             0.0139\n",
      "\n",
      "\n",
      "Top 5 documents according to Log Normalization Weighting Scheme: \n",
      "\n",
      "               Cosine_Similarity\n",
      "Doc_Id                          \n",
      "cranfield0634             0.1990\n",
      "cranfield0001             0.1528\n",
      "cranfield0011             0.1359\n",
      "cranfield0284             0.1326\n",
      "cranfield0360             0.1279\n",
      "\n",
      "\n",
      "Top 5 documents according to Double Normalization Weighting Scheme: \n",
      "\n",
      "               Cosine_Similarity\n",
      "Doc_Id                          \n",
      "cranfield0634             0.1699\n",
      "cranfield0001             0.1545\n",
      "cranfield0284             0.1432\n",
      "cranfield0360             0.1281\n",
      "cranfield0011             0.1275\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query = \"experimental investigation aerodynamics\"\n",
    "query = input(\"Enter a query: \")\n",
    "relevant_docs(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc3246a",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9cc62a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_coeff(query):\n",
    "    jacc_coed_df = deepcopy(df)\n",
    "    jacc_coed_df.drop('Contents of file after Preprocessing',axis=1,inplace=True)\n",
    "\n",
    "    jacc_coef_list = []\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        doc = set(ast.literal_eval(df[\"Contents of file after Preprocessing\"][i]))\n",
    "        query = set(query)\n",
    "        \n",
    "        union = len(query.union(doc))\n",
    "        intersect = len(query.intersection(doc))\n",
    "\n",
    "        jacc_coef = intersect/union\n",
    "        jacc_coef_list.append(jacc_coef)\n",
    "\n",
    "    jacc_coed_df[\"Jaccard_Coefficient\"] = jacc_coef_list\n",
    "\n",
    "    jacc_coed_df.sort_values(\"Jaccard_Coefficient\",ascending=False,inplace=True)\n",
    "    jacc_coed_df = jacc_coed_df[:10]\n",
    "    print(\"Top 10 documents ranked by Jaccard coefficient value\\n\")\n",
    "    print(jacc_coed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "21bb2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 documents ranked by Jaccard coefficient value\n",
      "\n",
      "             Doc_Id  Jaccard_Coefficient\n",
      "765   cranfield0339             0.071429\n",
      "115   cranfield0932             0.068966\n",
      "79    cranfield1045             0.066667\n",
      "752   cranfield1227             0.054054\n",
      "680   cranfield0549             0.054054\n",
      "705   cranfield1083             0.054054\n",
      "182   cranfield0019             0.052632\n",
      "1096  cranfield0251             0.051282\n",
      "1061  cranfield0001             0.050847\n",
      "1217  cranfield0634             0.049180\n"
     ]
    }
   ],
   "source": [
    "#query = \"experimental investigation aerodynamics\"\n",
    "query = input(\"Enter a query: \")\n",
    "query = preprocess_query(query)\n",
    "compute_jaccard_coeff(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bcf60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
